[["fnirs-data-analysis-pipeline.html", "Psychology Statistics and Codes Notebook Chapter 1 fNIRS data analysis pipeline 1.1 Individual Level Analysis 1.2 Group Level Results: Linear mixed model (LMM)", " Psychology Statistics and Codes Notebook Qiuyu Yu 2024-11-24 Chapter 1 fNIRS data analysis pipeline library(dplyr) library(readr) library(tidyverse) library(broom) library(purrr) library(tidyr) library(emmeans) data &lt;- read_csv(&quot;E:\\\\Research Projects\\\\YDI\\\\fNIRS\\\\DORRY_fNIRS_CLEAN_TIME.SERIES_Y.csv&quot;) data &lt;- data %&gt;% mutate( Game = ifelse(is.na(Game), 0, Game), Valance = ifelse(is.na(Valance), 0, Valance) ) 1.1 Individual Level Analysis 1.1.1 General Linear Model When we finally extract the data, or matrix, for the fNIRS data, we get matrix including ID, channels, Condition(factor), Time. Next, we perform GLM analysis on the observed data for each channel. Every channel’s formula will be look like: \\[ Y = \\sum \\beta _{i}x_{i} + \\epsilon = X\\beta + \\epsilon \\] According to the GLM formula, Y is the observed data matrix (fNIRS signal, time series data T). Y is a vector of T*1. X represents the design matrix of \\(x_{i}\\), and \\(x_{i}\\) contains stimuli, conditions, confounding factors(eg. noise), etc. X is a matrix of T*i. The matrix will be look like: \\[ \\left[ \\begin{array}{c} Y_{1} \\\\ Y_{2} \\\\ \\vdots \\\\ Y_{T} \\end{array} \\right] = \\left[ \\begin{array}{cccc} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1i} \\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2i} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{T1} &amp; x_{T2} &amp; \\cdots &amp; x_{Ti} \\end{array} \\right] \\left[ \\begin{array}{c} \\beta_{1} \\\\ \\beta_{2} \\\\ \\vdots \\\\ \\beta_{i} \\end{array} \\right] + \\left[ \\begin{array}{c} \\epsilon_{1} \\\\ \\epsilon_{2} \\\\ \\vdots \\\\ \\epsilon_{T} \\end{array} \\right] \\] The estimates of \\(\\beta\\) represent the coefficients of the GLM, which is what we want.Finding \\(\\beta\\) by the least squares method. \\[ \\hat{\\beta } = (X^{T}X)^{-1}X^{T}Y \\] Next we can construct dynamic blood flow response maps based on convolution of experimental stimulus conditions. Since the principles of fMRI and fNIRS are almost the same, more detailed GLM and HRF construction can be referred to fMRI. https://mriquestions.com/general-linear-model.html Convolution of 3 Events 1.1.1.1 Syntax Define the function for GLM. fNIRS_GLM &lt;- function(ID, channel, input_data) { tryCatch({ glm_model &lt;- glm(as.formula(paste0(&quot;`&quot;, channel, &quot;` ~ Valance&quot;)), data = input_data) glm_summary &lt;- tidy(glm_model) model_stats &lt;- glance(glm_model) glm_summary &lt;- glm_summary %&gt;% mutate(Channel = channel, ID = ID) %&gt;% mutate(df = model_stats$df.residual, se = std.error, t = statistic, significant = p.value &lt; 0.05) return(glm_summary) }, error = function(e) { return(tibble( estimate = NA_real_, std.error = NA_real_, statistic = NA_real_, p.value = NA_real_, Channel = channel, ID = ID, df = NA_real_, se = NA_real_, t = NA_real_, significant = NA )) }) } Create the list for unique IDs and channels: unique_ids &lt;- unique(data$ID) channel_columns &lt;- grep(&quot;^S&quot;, colnames(data), value = TRUE) perform the GLM function: results &lt;- map_df(unique_ids, function(ID) { map_df(channel_columns, function(channel) { input_data &lt;- data %&gt;% filter(ID == !!ID) fNIRS_GLM(ID, channel, input_data) }) }) head(results, 10) ## # A tibble: 10 × 11 ## term estimate std.error statistic p.value Channel ID df se t ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Inte… -0.164 0.595 -0.276 0.783 S1_D1 … 1001 2779 0.595 -0.276 ## 2 Valan… 8.82 2.93 3.01 0.00264 S1_D1 … 1001 2779 2.93 3.01 ## 3 Valan… -4.57 2.93 -1.56 0.119 S1_D1 … 1001 2779 2.93 -1.56 ## 4 Valan… -3.03 2.95 -1.03 0.303 S1_D1 … 1001 2779 2.95 -1.03 ## 5 Valan… 3.22 2.92 1.10 0.270 S1_D1 … 1001 2779 2.92 1.10 ## 6 (Inte… -0.194 0.563 -0.345 0.730 S1_D8 … 1001 2779 0.563 -0.345 ## 7 Valan… 8.84 2.78 3.19 0.00146 S1_D8 … 1001 2779 2.78 3.19 ## 8 Valan… -8.12 2.78 -2.92 0.00348 S1_D8 … 1001 2779 2.78 -2.92 ## 9 Valan… -0.227 2.79 -0.0815 0.935 S1_D8 … 1001 2779 2.79 -0.0815 ## 10 Valan… 5.35 2.76 1.94 0.0529 S1_D8 … 1001 2779 2.76 1.94 ## # ℹ 1 more variable: significant &lt;lgl&gt; 1.1.2 Further Multiple Regressors: Contrasts Earlier we have obtained a vector of \\(\\beta\\) values corresponding to each experimental condition. Next, suppose we are interested in the contrast of positive vs. negative in the valance. We use “contrast vector” to quantify the individual experimental effects (think about the multifactorial experimental design). We extract the \\(c^{T} * \\beta\\) of the factor we are interested in. Then, we subtract them. \\[ c^{T}\\beta = \\begin{bmatrix}0 &amp; 1 &amp; ... &amp; 0\\end{bmatrix} \\begin{bmatrix} \\beta _{1}\\\\ \\beta _{2} \\\\... \\\\ \\beta _{i} \\end{bmatrix}= \\beta _{2}\\\\ ~\\\\ \\beta_{2}~contrast ~ to ~\\beta_{i}:\\\\ c^{T}\\beta = \\begin{bmatrix}0 &amp; 1 &amp; ... &amp; -1\\end{bmatrix} \\begin{bmatrix} \\beta _{1}\\\\ \\beta _{2} \\\\... \\\\ \\beta _{i} \\end{bmatrix}= \\beta _{2} - \\beta_{i} \\] Only simple comparisons are demonstrated here; depending on your experimental design you can perform more complex tests. Reference: Robert Luke, Eric D. Larson, Maureen J. Shader, Hamish Innes-Brown, Lindsey Van Yper, Adrian K. C. Lee, Paul F. Sowman, David McAlpine, “Analysis methods for measuring passive auditory fNIRS responses generated by a block-design paradigm,” Neurophoton. 8(2) 025008 (22 May 2021) https://doi.org/10.1117/1.NPh.8.2.025008 1.1.2.1 Syntax comparison_results &lt;- data.frame() for (ID in unique_ids) { for (Channel in channel_columns) { neutral_row &lt;- results[results$ID == ID &amp; results$Channel == Channel &amp; results$term == &quot;ValanceNeutFace&quot;, &quot;estimate&quot;] positive_row &lt;- results[results$ID == ID &amp; results$Channel == Channel &amp; results$term == &quot;ValancePosFace&quot;, &quot;estimate&quot;] if (length(neutral_row) &gt; 0 &amp;&amp; length(positive_row) &gt; 0) { contrast_estimate &lt;- neutral_row - positive_row comparison_results &lt;- rbind(comparison_results, data.frame( ID = ID, Channel = Channel, Contrast = &quot;Neutral - Positive&quot;, Estimate = contrast_estimate )) } } } head(comparison_results, 10) ## ID Channel Contrast estimate ## 1 1001 S1_D1 hbo Neutral - Positive -7.792860 ## 2 1001 S1_D8 hbo Neutral - Positive -13.464875 ## 3 1001 S1_D9 hbo Neutral - Positive -10.348189 ## 4 1001 S2_D1 hbo Neutral - Positive -5.387112 ## 5 1001 S2_D3 hbo Neutral - Positive -5.952495 ## 6 1001 S2_D9 hbo Neutral - Positive 1.780545 ## 7 1001 S3_D1 hbo Neutral - Positive -3.158280 ## 8 1001 S3_D3 hbo Neutral - Positive -6.933208 ## 9 1001 S4_D2 hbo Neutral - Positive -2.495172 ## 10 1001 S4_D3 hbo Neutral - Positive -5.174260 contrast by using emm comparison_results_2 &lt;- results %&gt;% filter(term != &quot;(Intercept)&quot;) %&gt;% group_by(Channel) %&gt;% do({ lm_model &lt;- lm(estimate ~ term, data = .) emm &lt;- emmeans(lm_model, ~ term) contrast_results &lt;- contrast(emm, method = &quot;pairwise&quot;) tidy(contrast_results) }) print(comparison_results_2) ## # A tibble: 576 × 9 ## # Groups: Channel [96] ## Channel term contrast null.value estimate std.error df statistic ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 S10_D11 hbo term ValanceNegFa… 0 -1.32 1.20 208 -1.10 ## 2 S10_D11 hbo term ValanceNegFa… 0 0.921 1.20 208 0.767 ## 3 S10_D11 hbo term ValanceNegFa… 0 0.412 1.20 208 0.343 ## 4 S10_D11 hbo term ValanceNeutF… 0 2.24 1.20 208 1.87 ## 5 S10_D11 hbo term ValanceNeutF… 0 1.73 1.20 208 1.45 ## 6 S10_D11 hbo term ValancePlace… 0 -0.509 1.20 208 -0.424 ## 7 S10_D11 hbr term ValanceNegFa… 0 1.89 1.21 208 1.56 ## 8 S10_D11 hbr term ValanceNegFa… 0 0.168 1.21 208 0.138 ## 9 S10_D11 hbr term ValanceNegFa… 0 1.18 1.21 208 0.972 ## 10 S10_D11 hbr term ValanceNeutF… 0 -1.72 1.21 208 -1.42 ## # ℹ 566 more rows ## # ℹ 1 more variable: adj.p.value &lt;dbl&gt; 1.1.3 Research of Interest (ROI) Depends on your research interest, define the cluster of ROI. Region-of-interest calculation was then performed by weighted averaging of the individual beta values across channels within each region, with weights equal to the inverse of the standard error of the GLM fit for each channel (Santosa et al., 2018). So we get GLM results, then we need to get weight for each channels so that we can have the value of ROI. \\[ \\omega _{i} = \\frac{1}{SE_{i}} \\] Now we have the weight for each channel. The value of each data point in the weighted average is multiplied by the corresponding weight, and then these products are summed and divided by the sum of the weights. \\[ \\frac{\\sum_{i=1}^{N}\\omega _{i}\\times \\beta _{i}}{\\sum_{j = 1}^{N} \\frac{1}{SE_{j}} } \\] Reference: Shader, M. J., Luke, R., Gouailhardou, N., &amp; McKay, C. M. (2021). The use of broad vs restricted regions of interest in functional near-infrared spectroscopy for measuring cortical activation to auditory-only and visual-only speech. Hearing Research, 406, 108256. https://doi.org/10.1016/j.heares.2021.108256 1.1.3.1 Syntax roi_channels &lt;- list( &quot;Inferior_frontal_R&quot; = c(&quot;S13_D11 hbo&quot;, &quot;S13_D11 hbr&quot;), &quot;Inferior_frontal_L&quot; = c(&quot;S5_D3 hbo&quot;, &quot;S5_D3 hbr&quot;), &quot;Middle_frontal_R&quot; = c(&quot;S9_D11 hbo&quot;, &quot;S9_D11 hbr&quot;, &quot;S10_D11 hbo&quot;, &quot;S10_D11 hbr&quot;, &quot;S12_D11 hbo&quot;, &quot;S12_D11 hbr&quot;, &quot;S12_D12 hbo&quot;, &quot;S12_D12 hbr&quot;, &quot;S14_D12 hbo&quot;, &quot;S14_D12 hbr&quot;), &quot;Middle_frontal_L&quot; = c(&quot;S2_D3 hbo&quot;, &quot;S2_D3 hbr&quot;, &quot;S3_D3 hbo&quot;, &quot;S3_D3 hbr&quot;, &quot;S4_D2 hbo&quot;, &quot;S4_D2 hbr&quot;, &quot;S4_D3 hbo&quot;, &quot;S4_D3 hbr&quot;, &quot;S4_D5 hbo&quot;, &quot;S4_D5 hbr&quot;, &quot;S6_D2 hbo&quot;, &quot;S6_D2 hbr&quot;, &quot;S6_D5 hbo&quot;, &quot;S6_D5 hbr&quot;), &quot;Superior_frontal_R&quot; = c(&quot;S1_D8 hbo&quot;, &quot;S1_D8 hbr&quot;, &quot;S1_D9 hbo&quot;, &quot;S1_D9 hbr&quot;, &quot;S9_D8 hbo&quot;, &quot;S9_D8 hbr&quot;, &quot;S9_D9 hbo&quot;, &quot;S9_D9 hbr&quot;, &quot;S11_D9 hbo&quot;, &quot;S11_D9 hbr&quot;, &quot;S11_D10 hbo&quot;, &quot;S11_D10 hbr&quot;, &quot;S12_D10 hbo&quot;, &quot;S12_D10 hbr&quot;, &quot;S14_D10 hbo&quot;, &quot;S14_D10 hbr&quot;, &quot;S14_D4 hbo&quot;, &quot;S14_D4 hbr&quot;, &quot;S11_D4 hbo&quot;, &quot;S11_D4 hbr&quot;), &quot;Superior_frontal_L&quot; = c(&quot;S1_D1 hbo&quot;, &quot;S1_D1 hbr&quot;, &quot;S1_D9 hbo&quot;, &quot;S1_D9 hbr&quot;, &quot;S11_D2 hbo&quot;, &quot;S11_D2 hbr&quot;, &quot;S11_D4 hbo&quot;, &quot;S11_D4 hbr&quot;, &quot;S11_D9 hbo&quot;, &quot;S11_D9 hbr&quot;, &quot;S2_D1 hbo&quot;, &quot;S2_D1 hbr&quot;, &quot;S2_D9 hbo&quot;, &quot;S2_D9 hbr&quot;, &quot;S6_D2 hbo&quot;, &quot;S6_D2 hbr&quot;, &quot;S6_D4 hbo&quot;, &quot;S6_D4 hbr&quot;), &quot;Remainder_parietal_lobe_R&quot; = c(&quot;S15_D14 hbo&quot;, &quot;S15_D14 hbr&quot;, &quot;S15_D15 hbo&quot;, &quot;S15_D15 hbr&quot;, &quot;S16_D14 hbo&quot;, &quot;S16_D14 hbr&quot;, &quot;S16_D15 hbo&quot;, &quot;S16_D15 hbr&quot;), &quot;Remainder_parietal_lobe_L&quot; = c(&quot;S7_D6 hbo&quot;, &quot;S7_D6 hbr&quot;, &quot;S7_D7 hbo&quot;, &quot;S7_D7 hbr&quot;, &quot;S8_D6 hbo&quot;, &quot;S8_D6 hbr&quot;, &quot;S8_D7 hbo&quot;, &quot;S8_D7 hbr&quot;) ) ROI_list &lt;- c(&quot;Inferior_frontal_R&quot;, &quot;Inferior_frontal_L&quot;, &quot;Middle_frontal_R&quot;, &quot;Middle_frontal_L&quot;, &quot;Superior_frontal_R&quot;, &quot;Superior_frontal_L&quot;, &quot;Remainder_parietal_lobe_R&quot;, &quot;Remainder_parietal_lobe_L&quot;) We have defined the cluster of ROI and get the GLM results for each channel and participant. We pull out part of the results and calculate the weight first. ROI_results &lt;- data.frame() for (id in unique_ids) { # filter each ID&#39;s each ROI id_results &lt;- results %&gt;% filter(ID == id) for (roi in ROI_list) { roi_channels_list &lt;- roi_channels[[roi]] # add Chroma channel roi_results &lt;- id_results %&gt;% filter(Channel %in% roi_channels_list) %&gt;% mutate(Chroma = ifelse(grepl(&quot;hbo$&quot;, Channel), &quot;hbo&quot;, &quot;hbr&quot;)) weighted_results &lt;- roi_results %&gt;% group_by(term, Chroma) %&gt;% mutate(weight = 1 / se) %&gt;% mutate(weight = weight / sum(weight, na.rm = TRUE)) %&gt;% summarise( Weighted_Beta = sum(estimate * weight, na.rm = TRUE), se = sqrt(sum((weight^2) * se^2, na.rm = TRUE)), t = Weighted_Beta / se, dfe = mean(df, na.rm = TRUE), # df for each channel p = 2 * pt(-abs(t), df = dfe), .groups = &#39;drop&#39; ) weighted_results &lt;- weighted_results %&gt;% mutate(p = format(p, scientific = FALSE, digits = 4)) weighted_results &lt;- weighted_results %&gt;% mutate(ROI = roi, ID = id) ROI_results &lt;- rbind(ROI_results, weighted_results) } } head(ROI_results, 10) ## # A tibble: 10 × 9 ## term Chroma Weighted_Beta se t dfe p ROI ID ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) hbo 0.0980 0.545 0.180 2779 0.85737 Inferi… 1001 ## 2 (Intercept) hbr -0.144 0.548 -0.264 2779 0.79201 Inferi… 1001 ## 3 ValanceNegFace hbo 1.73 2.69 0.646 2779 0.51856 Inferi… 1001 ## 4 ValanceNegFace hbr -3.40 2.70 -1.26 2779 0.20827 Inferi… 1001 ## 5 ValanceNeutFace hbo -5.54 2.69 -2.06 2779 0.03941 Inferi… 1001 ## 6 ValanceNeutFace hbr 0.420 2.70 0.156 2779 0.87640 Inferi… 1001 ## 7 ValancePlace hbo -3.97 2.70 -1.47 2779 0.14196 Inferi… 1001 ## 8 ValancePlace hbr 1.36 2.71 0.502 2779 0.61569 Inferi… 1001 ## 9 ValancePosFace hbo 5.71 2.67 2.14 2779 0.03283 Inferi… 1001 ## 10 ValancePosFace hbr 5.26 2.69 1.96 2779 0.05064 Inferi… 1001 1.1.4 Averaged Wave Analysis https://mne.tools/mne-nirs/stable/auto_examples/general/plot_16_waveform_group.html 1.2 Group Level Results: Linear mixed model (LMM) Normally applies to between subjects’ analysis.Advanced alternative to t-test or ANOVA. Although there are different alternatives for these analyses, the LMM approach used in this study has three main advantages over the other methods such as t-test, ANOVA, or classical linear regression model as it provides: (i) to control type-I error rate by allowing all the variables at the same time, and (ii) handling correlation among the repeated measures which is not possible with the other mentioned methods as their implementation requires the independence among observations in the data, and (iii) to deal with the complexity of the neural data sets, including many levels in which some of their categorical responses relate to the structure of brain imaging technologies. Further, as many neural data tend to include observations that behave differently than the rest of the data points due to motion artifacts or uncontrolled errors, the influence of outliers is aimed to be decreased during the model parameter estimates through a robust LMM approach. \\[ Y_{i} = X_{i}\\beta + Z_{i}u_{i} + \\varepsilon _{i}\\\\ u_{i} ~ N(0,D)\\\\ \\varepsilon _{i} ~ N(0,R_{i}) \\] Similar to GLM, \\(X_{i}\\beta\\) is the fixed effect, while \\(Z_{i}u_{i}\\) and \\(\\varepsilon _{i}\\) are the random effects. Reference: Cakar, S., &amp; Gokalp Yavuz, F. (2023). Nested and robust modeling techniques for fNIRS data with demographics and experiment related factors in n-back task. Neuroscience Research, 186, 59-72. https://doi.org/10.1016/j.neures.2022.10.007 1.2.1 Syntax library(lme4) library(broom.mixed) 1.2.1.1 Weighted_Beta ~ -1 + ROI:term:Chroma + (1|ID) This analysis is more like ANOVA, but adding the random effect of participants. Examine the effect of each ROI on the weighted beta under different conditions and taking into account the difference between HbO and HbR. How different brain regions (ROIs) respond differently to hbo and hbr under different experimental conditions (term), respectively; participants as random effect. Beta values were then averaged across the four broad regions of interest. These data were then entered into a robust linear mixed-effects model to evaluate the group-level responses for each stimulus condition divided across four regions of interest. lmm_model &lt;- lmer(Weighted_Beta ~ -1 + ROI:term:Chroma + (1|ID), data = ROI_results) model_summary &lt;- summary(lmm_model) model_summary1 &lt;- tidy(lmm_model) model_summary1 &lt;- model_summary1 %&gt;% mutate(p.value = 2 * (1 - pnorm(abs(estimate / std.error)))) Shader, M. J., Luke, R., Gouailhardou, N., &amp; McKay, C. M. (2021). The use of broad vs restricted regions of interest in functional near-infrared spectroscopy for measuring cortical activation to auditory-only and visual-only speech. Hearing Research, 406, 108256. https://doi.org/10.1016/j.heares.2021.108256 1.2.1.2 channel_data ~ Valance + ACC + ART + (1 | ID) + (1 | Channel:ID) (1 | Channel:ID) is the random effect associated with the intercept for each channel within-subject. Each channel (Channel) for each participant (ID) has its own different random intercept: it is assumed that the responses for each channel may be different for each person. Theoretically you could add other conditions (or main effects), interaction effects, and random effects, and then trim the model based on whether or not they are significant, but this is pretty data-driven. long_data &lt;- data %&gt;% pivot_longer(cols = all_of(channel_columns), names_to = &quot;Channel&quot;, values_to = &quot;channel_data&quot;) lmm_model2 &lt;- lmer(channel_data ~ Valance + ACC + ART + (1 | ID) + (1 | Channel:ID), data = long_data) ## Warning: Some predictor variables are on very different scales: consider ## rescaling ## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = control$checkConv, : Model is nearly unidentifiable: very large eigenvalue ## - Rescale variables? lmm_summary2 &lt;- tidy(lmm_model2) LMM_results &lt;- lmm_summary2 %&gt;% select(term, estimate, std.error, statistic) %&gt;% mutate(p.value = 2 * (1 - pnorm(abs(statistic))), significant = p.value &lt; 0.05) head(LMM_results, 10) ## # A tibble: 9 × 6 ## term estimate std.error statistic p.value significant ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 (Intercept) -3.42 0.241 -14.1 0 TRUE ## 2 ValanceNeutFace -0.111 0.0442 -2.52 0.0117 TRUE ## 3 ValancePlace -0.545 0.0448 -12.2 0 TRUE ## 4 ValancePosFace -0.726 0.0440 -16.5 0 TRUE ## 5 ACC 2.88 0.213 13.5 0 TRUE ## 6 ART 0.00120 0.0000955 12.6 0 TRUE ## 7 sd__(Intercept) 1.44 NA NA NA NA ## 8 sd__(Intercept) 0.494 NA NA NA NA ## 9 sd__Observation 20.1 NA NA NA NA Cakar, S., &amp; Gokalp Yavuz, F. (2023). Nested and robust modeling techniques for fNIRS data with demographics and experiment related factors in n-back task. Neuroscience Research, 186, 59-72. https://doi.org/10.1016/j.neures.2022.10.007 You can replace channel_data with channels for ROI. We can include other covariates, such as sex, to do further analysis for ROI to compare different conditions. ROI_results_compare &lt;- ROI_results %&gt;% filter(str_detect(ROI, &quot;Inferior_frontal_R&quot;)) %&gt;% filter(term %in% c(&quot;ValanceNeutFace&quot;, &quot;ValancePlace&quot;)) %&gt;% filter(Chroma == &quot;hbo&quot;) %&gt;% group_by(ID) %&gt;% filter(n() &gt; 1) %&gt;% ungroup() print(ROI_results_compare) ## # A tibble: 104 × 9 ## term Chroma Weighted_Beta se t dfe p ROI ID ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 ValanceNeutFace hbo -5.54 2.69 -2.06 2779 0.03941 Infe… 1001 ## 2 ValancePlace hbo -3.97 2.70 -1.47 2779 0.14196 Infe… 1001 ## 3 ValanceNeutFace hbo 4.47 2.33 1.92 2799 0.05498 Infe… 1003 ## 4 ValancePlace hbo -4.40 2.34 -1.88 2799 0.06051 Infe… 1003 ## 5 ValanceNeutFace hbo 4.37 2.53 1.73 2781 0.0843003 Infe… 1006 ## 6 ValancePlace hbo -3.17 2.54 -1.25 2781 0.2124539 Infe… 1006 ## 7 ValanceNeutFace hbo 1.83 2.84 0.645 3350 0.51884 Infe… 1011 ## 8 ValancePlace hbo 5.03 2.86 1.76 3350 0.07840 Infe… 1011 ## 9 ValanceNeutFace hbo -0.886 2.90 -0.306 3221 0.759945… Infe… 1018 ## 10 ValancePlace hbo -1.60 2.91 -0.551 3221 0.581978… Infe… 1018 ## # ℹ 94 more rows lmm_model_compare &lt;- lmer(Weighted_Beta ~ term + (1 | ID), data = ROI_results_compare) ## boundary (singular) fit: see help(&#39;isSingular&#39;) summary(lmm_model_compare) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: Weighted_Beta ~ term + (1 | ID) ## Data: ROI_results_compare ## ## REML criterion at convergence: 603.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.4520 -0.5756 -0.0736 0.6715 2.8783 ## ## Random effects: ## Groups Name Variance Std.Dev. ## ID (Intercept) 0.00 0.000 ## Residual 20.08 4.481 ## Number of obs: 104, groups: ID, 52 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 1.3654 0.6214 2.197 ## termValancePlace -1.4838 0.8788 -1.689 ## ## Correlation of Fixed Effects: ## (Intr) ## termVlncPlc -0.707 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) tidy(lmm_model_compare) ## # A tibble: 4 × 6 ## effect group term estimate std.error statistic ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 fixed &lt;NA&gt; (Intercept) 1.37 0.621 2.20 ## 2 fixed &lt;NA&gt; termValancePlace -1.48 0.879 -1.69 ## 3 ran_pars ID sd__(Intercept) 0 NA NA ## 4 ran_pars Residual sd__Observation 4.48 NA NA 1.2.1.3 channel_data ~ Game + Valance + (1 | ID) Still a little more advanced, GLM examines the effects of each experimental condition with participants on each channel from a holistic perspective. results_list &lt;- list() for (channel in channel_columns) { model_data &lt;- data %&gt;% select(Valance, !!sym(channel), ID) %&gt;% rename(channel_data = !!sym(channel)) lmm_model &lt;- lmer(channel_data ~ Valance + (1 | ID), data = model_data) lmm_summary &lt;- tidy(lmm_model) lmm_summary &lt;- lmm_summary %&gt;% select(term, estimate, std.error, statistic) %&gt;% mutate(p.value = 2 * (1 - pnorm(abs(statistic))), significant = p.value &lt; 0.05, Channel = channel) results_list[[channel]] &lt;- lmm_summary } results_each_channel &lt;- bind_rows(results_list) head(results_each_channel, 10) ## # A tibble: 10 × 7 ## term estimate std.error statistic p.value significant Channel ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; ## 1 (Intercept) 0.0343 0.0666 0.515 0.606 FALSE S1_D1 … ## 2 ValanceNegFace 0.621 0.341 1.82 0.0686 FALSE S1_D1 … ## 3 ValanceNeutFace 1.40 0.341 4.11 0.0000394 TRUE S1_D1 … ## 4 ValancePlace -1.15 0.344 -3.34 0.000824 TRUE S1_D1 … ## 5 ValancePosFace -1.71 0.338 -5.05 0.000000442 TRUE S1_D1 … ## 6 sd__(Intercept) 0 NA NA NA NA S1_D1 … ## 7 sd__Observation 24.2 NA NA NA NA S1_D1 … ## 8 (Intercept) 0.0492 0.0623 0.791 0.429 FALSE S1_D8 … ## 9 ValanceNegFace -0.560 0.318 -1.76 0.0783 FALSE S1_D8 … ## 10 ValanceNeutFace 1.51 0.318 4.73 0.00000220 TRUE S1_D8 … "],["linear-and-non-linear-sem.html", "Chapter 2 Linear and Non-linear SEM 2.1 What is the direct and indirect effect? 2.2 Nonlinear SEM from a math way 2.3 How to caculate the slope? 2.4 Introducing the confounding variable W 2.5 what is the relationship between moderated mediation and Nonlinear SEM? 2.6 Practice in R: Cross-sectional 2.7 If X is dichotomous (draft) 2.8 Which model fit best for my theory? 2.9 Reference", " Chapter 2 Linear and Non-linear SEM SEM is built on factor analysis and regression, and I’d recommend reviewing regression before you start. Of course it’s fine if you don’t have a lot of knowledge in this area, from an application standpoint you just need to understand the indicators as well as interpret the results. we use simple slope to get the path effect and calculate mediation effect, so it is very simple to label these effects. Let us starts with simple mediation model: \\[ X\\to M: \\\\ M = aX + i _{1} \\\\ ~\\\\ Mediation(indirect)~Path: X\\to M \\to Y: \\\\ Y = bM + i _{2} = b(aX + i _{1}) + i _{2} = abX + bi _{1} + i _{2}\\\\ ~\\\\ Direct ~ Path: X \\to Y:\\\\ Y = cX + i _{3}\\\\ ~\\\\ Total ~ Effect: \\\\ Y = bM + i _{2} + cX + i _{3} = (ab + c)X + i \\\\ \\] 2.1 What is the direct and indirect effect? Direct effect: such as X - Y, X - M, M - Y, reveals the independent effect of the independent variable, will not influence by any other mediators. Indirect effect: such as X - M - Y, is X influence M and thus has impact on Y, which quantified as the product of a(X - M) and b (M - Y). When X changes by one unit, eventually Y will change ab units. Here, the slope of f(X) is what we want, representing the linear relationship between the variables because it quantifies the rate of change. We can get the slope by derivation. Let’s review the simple inverse rule first \\[ f(X) = x^{n} \\\\ f&#39;(x) = n\\times x^{n-1} ~\\\\ Instantaneous ~ rate ~ of ~ change = \\frac{\\mathrm{d} y}{\\mathrm{d} x} \\\\ Y = log_{e}(X) = In(X)\\\\ e^{Y} = X\\\\ Derivation ~ of ~ this ~ equation:\\\\ 1 = e^{Y}\\frac{\\mathrm{d} y}{\\mathrm{d} x} \\\\ Y&#39; = \\frac{1}{X} \\] Next, let’s derive the functions of M and Y. \\[ X\\to M: \\\\ M&#39;(X) = a\\\\ ~\\\\ Mediation(indirect)~Path: X\\to M \\to Y: \\\\ Y&#39;(X) = ab\\\\ ~\\\\ Direct ~ Path: X \\to Y:\\\\ Y&#39;(X) = c\\\\ ~\\\\ Total ~ Effect: \\\\ Y&#39;(X) = ab + c \\] Hence, we will know the indirect effect is ab, direct effect is c, and the total effect is ab + c. Depending on your theory, your model will change, and therefore the formula for the effect value will change, but you can derive the specific effect value from the formula. Note that since there is a causal relationship between X and M, M is not independent; X is not affected by any other factor and thus it is independent. Therefore, when estimating the indirect effect of X on Y, M should not be viewed as an independent variable, but rather transformed in the form of X. 2.2 Nonlinear SEM from a math way when we are talking about nonlinear SEM, what we are talking about? some of them may be qudric or cubic, or looks like Y = e^X the plot will look like: Parabola x &lt;- seq(-1, 1, by = 0.01) data &lt;- data.frame(x = x, y = -x^2 + x + 1) ggplot(data, aes(x = x, y = y)) + geom_line(color = &quot;deepskyblue&quot;, size = 1) + labs(title = &quot;y = -x^2 + x + 1&quot;, x = &quot;x&quot;, y = &quot;y&quot;) + theme_minimal() + theme( plot.title = element_text(hjust = 0.5, size = 18), axis.title = element_text(size = 14), axis.text = element_text(size = 12) ) S-shaped curve x &lt;- seq(-1, 1, by = 0.01) data &lt;- data.frame(x = x, y = -x^3 + x^2 +x + 1) ggplot(data, aes(x = x, y = y)) + geom_line(color = &quot;deepskyblue&quot;, size = 1) + labs(title = &quot;y = y = -x^3 + x^2 +x + 1&quot;, x = &quot;x&quot;, y = &quot;y&quot;) + theme_minimal() + theme( plot.title = element_text(hjust = 0.5, size = 18), axis.title = element_text(size = 14), axis.text = element_text(size = 12) ) 2.3 How to caculate the slope? Apparently we cannot just use ab as the indirect path effect, because the relationships between these variables are not linear. However,the derivative can be used to find the slope, or instantaneous slope, of a curve at a point. 2.3.1 Quadratic curve Let’s start by assuming that the relationship between X and M and Y is quadratic. \\[ X\\to M: \\\\ M = a_{2}X^{2} + a_{1}X + i _{1} \\\\ ~\\\\ Mediation(indirect)~Path: X\\to M \\to Y: \\\\ Y = bM + i _{2} = b(a_{2}X^{2} + a_{1}X + i _{1}) + i _{2} = a_{1}bX + a_{2}bX^{2} + bi _{1} + i _{2}\\\\ ~\\\\ Direct ~ Path: X \\to Y:\\\\ Y = c_{2}X^{2} + c_{1}X + i _{3}\\\\ ~\\\\ Total ~ Effect: \\\\ Y = bM + i _{2} + c_{2}X^{2} + c_{1}X + i _{3} = (a_{1}b + c_{1})X + (a_{2}b + c_{2})X^2 + i _{2} + i _{3} \\\\ \\] Next perform the derivation. \\[ X\\to M: \\\\ M&#39;(X) = 2a_{2}X + a_{1}\\\\ ~\\\\ Mediation(indirect)~Path: X\\to M \\to Y: \\\\ Y&#39;(X) = a_{1}b + 2a_{2}bX = (a_{1}+2a_{2}X)b\\\\ ~\\\\ Direct ~ Path: X \\to Y:\\\\ Y&#39;(X) = 2c_{2}X + c_{1}\\\\ ~\\\\ Total~Effect: Y&#39;(X) = (a_{1}b + c_{1}) + 2(a_{2}b + c_{2})X \\] 2.3.2 Cubic Curve The following derivation will omit the constant term. \\[ X\\to M: \\\\ M = a_{3}X^3 + a_{2}X^{2} + a_{1}X \\\\ ~\\\\ Mediation(indirect)~Path: X\\to M \\to Y: \\\\ Y = bM + i _{2} = (a_{3}b + c_{3})X^3 + (a_{2}b + c_{2})X^{2} + (a_{1}b + c_{1})X\\\\ ~\\\\ Direct ~ Path: X \\to Y:\\\\ Y = c_{3}X^{3} + c_{2}X^{2} + c_{1}X\\\\ \\] Next perform the derivation. \\[ X\\to M: \\\\ M&#39;(X) = 3a_{3}X^2 + 2a_{2}X + a_{1}\\\\ ~\\\\ Mediation(indirect)~Path: X\\to M \\to Y: \\\\ Y&#39;(X) = (a_{1}b + c_{1}) + 2X(a_{2}b + c_{2}) + 3X^2(a_{3}b + c_{3})\\\\ ~\\\\ Direct ~ Path: X \\to Y:\\\\ Y&#39;(X) = 3c_{3}X^2 + 2c_{2}X + c_{1} \\] 2.4 Introducing the confounding variable W When the relationship between X - M and M - Y is linear, W does not affect the indirect effect since the constant term W will be omitted in the derivation. These relations are fixed for the indirect effect regardless of the value of W after controlling for W. However, when the M - Y relationship is nonlinear, W will affect the indirect effect. \\[ M = aX + d_{1}W\\\\ Y = b_{1}M + b_{2}M^2 + d_{2}W\\\\ Y&#39; = 2a^2b_{2}X + 2ab_{2}d_{1}W + ab_{1} \\] 2.5 what is the relationship between moderated mediation and Nonlinear SEM? https://gabriellajg.github.io/EPSY-579-R-Cookbook-for-SEM/ knitr::include_graphics(&quot;nonlinear SEM.png&quot;) Figure 2.1: This is a caption for the image 2.6 Practice in R: Cross-sectional 2.6.1 Centering and Scaling Centering and scaling are done to change the intercept and slope, respectively, and the main purpose of doing so is so that the results can be interpreted better. For example, we sometimes care more about the trend than the baseline level, so we can subtract the value of the baseline level, which is centering. Moreover, centering can reduce nonessential multicollinearity. Scaling allows all variables to be compared at the same level, similar to ” standardization”. 2.6.2 Syntax in R Because the model is non-linear, so if you want to interpret the slope, normally we will use “representative values”, M±SD, which is similar to the interactions or moderators. Alternatives might be the 25th, 50th, and 75th percentiles. X2 is the quadratic of X, if you want to do the further analysis in Mplus you may need to do transform x^2 to X2. Or you can just use (X^2) in R. NLModel &lt;- &#39; Y ~ b * M + c1 * X + c2 * X2 M ~ a1 * X + a2 * X2 M ~ 1 Low := -1.5 Mean := 0 High := 1.5 # slope aLow := a1 + 2 * a2 * Low aMean := a1 + 2 * a2 * Mean aHigh := a1 + 2 * a2 * High # define indirect effect abLow := b * aLow abMean := b * aMean abHigh := b * aHigh &#39; set.seed(100) NLModel.fit = sem(NLModel, data = df, test=&quot;scaled.shifted&quot;, estimator=&quot;MLR&quot;, fixed.x = FALSE, se=&quot;bootstrap&quot;, bootstrap=1000, meanstructure = TRUE, conditional.x = FALSE) summary(NLModel.fit, standardized = TRUE, fit.measures = TRUE) parameterEstimates(NLModel.fit, boot.ci.type = &quot;perc&quot;, ci = TRUE, level = 0.95) # extract parameters params &lt;- parameterEstimates(NLModel.fit) i1 &lt;- params[params$lhs == &quot;M&quot; &amp; params$op == &quot;~1&quot;, &quot;est&quot;] # extract path coefficient a1 &lt;- params[params$label == &quot;a1&quot;, &quot;est&quot;] a2 &lt;- params[params$label == &quot;a2&quot;, &quot;est&quot;] # calculate mediation M MLow &lt;- i1 + a1 * Low + a2 * Low^2 MMean &lt;- i1 + a1 * Mean + a2 * Mean^2 MHigh &lt;- i1 + a1 * High + a2 * High^2 MLow MMean MHigh 2.7 If X is dichotomous (draft) First, use dummy code (0, 1). Second, “quantify the indirect effect of X on Y via M as the instantaneous indirect effect when X is at its lowest coded value.” Consider we are using bootstrap, the inference or results will not change. \\[ Logit ~ Regression:\\\\ logit(Y) = log(\\frac{P(Y = 1)}{1-P(Y = 1)}) = log(\\frac{P(Y = 1)}{P(Y = 0)})= bM + c&#39;X + \\varepsilon_{Y}\\\\ ~\\\\ Probit~Regression:\\\\ \\Phi^{-1} (P(Y = 1)) = bM + c&#39;X + \\varepsilon_{Y} \\] logit(Y) is a logistic regression transformation representing the log odds of Y=1 occurring, and P(Y = 1) means the chance of Y=1 occurring; the latter is the inverse function of the cumulative distribution function of the normal distribution and is used in the Probit model. $$ log() = b_{0} + b_{1}x\\ = e^{b_{0} + b_{1}x} \\ p = = $$ CDE: controlled direct effect, we control the M so that we can get direct effect. DE: direct effect $$ CDE(m) = E[Y(1,m) - Y(0, m)| C = c]\\ DE = E[Y(1, M(0)) - Y (0, M(0)) | C = c] = \\ _{-}^{} { E[Y|C = c, X = 1, M = m] - E[Y|C = c, X = 0, M = m] } f(M|C = c, X = 0)M\\ $$ Total indirect \\[ TIE = E[Y(1,M(1)) - Y(1, M(0))|C = c]\\\\ = \\int_{-\\infty }^{\\infty } E[Y|C = c, X = 1, M = m] \\times f(M|C =c, X = 1) \\partial M - \\int_{-\\infty }^{\\infty } E[Y|C = c, X = 1, M = m] \\times f(M|C =c, X = 0) \\partial M \\] Total effect: \\[ TE = E[Y(1) - Y(0)|C = c]\\\\ =E[Y(1, M(1)) - Y(0, M(0))|C = c] \\] pure indirect effect \\[ PIE = E[Y(0,M(1)) - Y(0,M(0))| C = c] \\] \\[ probit(x, x^{*} ) = \\frac{[\\beta _{0} + \\beta _{2}x + (\\beta _{1} + \\beta _{3}x)(\\gamma _{0} + \\gamma _{1}x^{*})]}{\\sqrt{v(x)} } \\] variance \\[ v(x) = (\\beta _{1} + \\beta _{3}x)^{2} \\sigma _{2}^{2} + 1 \\] We will use Average Causal Mediation Effect (ACME) and Monte Carlo simulation to calculate the mediation effect. \\[ indirect ~ effect:\\\\ IE(x) = P(Y = 1 | X=x, M = M(x)) - P(Y = 1 | X = x, M = M(x^{*}))\\\\ ACME = \\frac{1}{n} \\sum_{i=1}^{n}(P(Y_{i} = 1 |X_{i}, M(X_{i})) - P(Y_{i} = 1 | X_{i}, M(_{i}^{*} ))) \\] 2.8 Which model fit best for my theory? There are some ways you can decide use which model. The model you build is never 100% correct or perfect. State the reason when you decide to use a certain model.Complex models may not be the best. Based on your theory or experience. Based on the previous literature. Data driven: exploratory analysis. Build up several models and check the model fit, R square, plot, or do the model comparison. 2.9 Reference Hayes, A. F., &amp; Preacher, K. J. (2010). Quantifying and Testing Indirect Effects in Simple Mediation Models When the Constituent Paths Are Nonlinear. Multivariate Behavioral Research, 45(4), 627ג660. https://doi.org/10.1080/00273171.2010.498290 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
